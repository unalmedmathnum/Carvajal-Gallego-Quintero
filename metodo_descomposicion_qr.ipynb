{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc595f3-29de-431d-965c-2d5fa298a0d0",
   "metadata": {},
   "source": [
    "## Método QR\n",
    "Trabajemos desde cero, viendo el comportamiento del método potencia en el autoespacio generado por el primer autovector, como podemos generalizar esto a autoespacios más grandes, y como QR nos permite encontrar estos autoespacios, y con ellos, los autovalores de una matriz.\n",
    "En toda la discusión, supondremos lo siguiente: \n",
    "- Los autovalores de $A$ son $\\lambda_1 , \\lambda_2,...,\\lambda_n$, ordenados según su módulo de forma decreciente\n",
    "- Los autovectores $v_1,v_2,...,v_n$ de $A$ forman una base para $\\mathbb{R}^n$\n",
    "- $A$ es invertible\n",
    "\n",
    "Al usar método potencia, tomamos un vector inicial $x = c_1 v_1 + c_2 v_2 + ... + c_n v_n$. Para que funcione, necesitaremos que $c_1 \\neq 0$, y aplicaremos $A$ a $x$, obteniendo $$A^m x= c_1 \\lambda_1^m v_1 + ... + c_n \\lambda_n^m v_n$$\n",
    "Si nosotros conocieramos $\\lambda_1$ y supieramos que $|\\lambda_1| > |\\lambda_2|$, tendriamos que \n",
    "$$ (1/\\lambda_1)^m A^m x = c_1 v_1 + c_2 (\\lambda_2 / \\lambda_1)^m v_2 + ... + c_n (\\lambda_n / \\lambda_1)^m v_n $$\n",
    "Como $|\\lambda_k / \\lambda_1|^m \\rightarrow 0 $ si $m \\rightarrow \\infty$ par cualquier $k$, mientras más iteremos, obtenemos un vector cuya direccion se acerca cada vez más y más a $v_1$, mientras que las direcciones en los otros vectores $v_2,...,v_n$ se hacen arbitrariamente pequeñas. \n",
    "\n",
    "Si en vez de pensar en $A^m x$, llamamos $S = \\langle x \\rangle$, podemos ver cada paso como la elección de un representante de $A^m S = \\langle A^m x \\rangle$, y la convergencia se puede apreciar en que el ángulo o distancia entre los espacios vectoriales $A^m S$ y $\\langle v_1 \\rangle$ tiende a 0 Aunque todavia no tenemos una _definicón_ de estos ángulos o distancias, un ejemplo puede ayudar a verlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52427481-727a-49d2-831e-cbcfba2e42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Mostrar un ejemplo con A=[[4,2],[1,3]], autovalores 5 y 2, y autovectores [2,1] y [-1,1], ver si en saage o con pantallazos de geogebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef18e0d-8a94-4a9c-b652-ebedcf2d3811",
   "metadata": {},
   "source": [
    "Ahora, que pasa si queremos encontrar autoespacios más grandes? LLamemos $T_k = \\langle v_1, ... ,v_k \\rangle$ y $U_k = \\langle v_{k+1} , ... ,v_n \\rangle$. Comencemos con un espacio $S_k$ que aunque podria ser con la dimensión que quiera, por simplicidad tomemos $S = \\langle x \\rangle $ con la condición que $S_k \\cap U_k = \\langle 0 \\rangle$, equivalente a decir que las primeras $k$ coordenadas de todo vector de $S_k$ en la base de $v_1,...,v_n$ son distintas a 0, y que $|\\lambda_k| > |\\lambda_{k+1}|$. Asi, tendriamos que\n",
    "\\begin{align*}\n",
    " (1/\\lambda_k)^m A^m x = &c_1 (\\lambda_1 / \\lambda_k)^m v_1 + ... + c_k v_k &&\\text{Componente en } T_k \\\\\n",
    " &+ c_{k+1} (\\lambda_{k+1}/\\lambda_k)^m v_{k+1} + ... + c_n (\\lambda_n / \\lambda_k)^m v_n &&\\text{Componente en } U_k\n",
    "\\end{align*}\n",
    "Y como $|\\lambda_i /\\lambda_k | <1 \\quad \\forall i > k$, tenemos que la componente en $U_k$ se hace cada vez más y más pequeña mientras iteramos $A^mS_k$, por lo que podriamos ver como se acerca a $T_k$.Paraa dimensiones arbitrarias, hacemos este proceso con cada aelemento de la base. De ahora en adelante, tomemos $S_k$ con dimensión $k$\n",
    "\n",
    "Si definimos la métrica entre subespcios vectoriales de $\\mathbb{R}^n$ como $d(S, T) = \\sup_{\\substack{s \\in S \\\\ ||s|| = 1}}\\{ \\inf_{t \\in T} \\|s - t\\|_2\\}$, se puede mostrar siguiendo la idea de la discusión anterior que bajo las hipótesis de que $S_k \\cap U_k = \\langle 0 \\rangle$, y $|\\lambda_k| > |\\lambda_{k+1}|$, entonces $d(A^m S_k,T_k) \\leq C \\cdot (\\lambda_{k+1}/\\lambda_k)^m \\quad(\\bigstar)$, con $C$ una constante. Esto será importantisimo más tarde.\n",
    "\n",
    "A la hora de ir calculando estos $A^m S_k$, como no podemos realmente calculr todo el espacio, trabajmos sobre una base inicial de $S_k$, digamos $\\{b_1,...,b_k\\}$, y vamos calculando $\\{A^m b_1, ... A^m b_r\\}$ que son bases de $A^m S_k$. Hay un problema, y es que todos estos elementos de la base van acercandose independientemente a elementos en $T_k$, mientras que su magnitud crece. Estaa base no es buena, y la mejor manera que podemos arreglarla es ortonormalizarla. Esto es, aplicamos Gram-Smith a la base mala $\\{A^m b_1, ... A^m b_r\\} $ para obtener una base buena (ortonormal) $\\{q_1^m , ... ,q_k^m\\} $\n",
    "\n",
    "**Primer hecho fundamental** : Si $\\{q_1 , ... ,q_k\\} $ es la ortogonalizción de la base $ \\{ b_1,...,b_k \\} $ para $S_k$, entonces $\\{q_1, ... , q_i\\}$ es un ortogonalización de la base $\\{b_1,...,b_i \\} $ para $S_i$, para cualquier $i \\leq k$. Gracias a esto, si hacemos $S_n = \\mathbb{R}^n = \\langle e_1, ... , e_n \\rangle$, y vamos iterando y encontrando bases buenas para $A^m S_n$, simultáneamente vamos encontrando basaes buenas para $A^m S_k \\quad \\forall k \\leq n$\n",
    "\n",
    "Bueno, y todo esto para que? Sabemos que con el subespacio inicial adecuado, podemos entonces encontrar grcias a un proceso de plicaación de $A$ y ortogonalización, bases para espacios vectoriales $A^m S_k$ arbitrariamente cercanas a los autoespacios $T_k$ gracias a $(\\bigstar)$. Pero aqui viene lo más importante. Si fijo $k$, y $Q = [ Q_1 | Q_2 ]$ es una matriz ortogonal cuyas primeras $k$ columnas $[Q_1]$ generan al autoespacio $T_k$, entonces se tiene lo siguiente:\n",
    "$$\n",
    "Q^T A Q =\\begin{bmatrix}\n",
    "    Q_1^T  \\\\ Q_2^T\n",
    "\\end{bmatrix} A \\begin{bmatrix}\n",
    "    Q_1 Q_2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    Q_1^T A Q^1 & Q_1^TAQ_2 \\\\ Q_2^TAQ_1 & Q_2^T A Q_2\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    A_{11} & A_{12}  \\\\ 0 & A_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Donde $A_{11} \\in \\mathbb{R}^{k \\times k} $, y $Q_2^T A Q_1 = Q_2^T Q_1 = 0$, por ser el espacio columna de $Q_1$ invariante bajo $A$, y ser $Q$ ortogonal.\n",
    "\n",
    "**Segundo hecho fundamental** : Si le pidieramos a $Q$ la condición de que sus para todo $k$, sus primeras $k$ columnas fueran una base de $T_k$, entonces tendríamos que $Q^T A Q$ es triangular superior, y como es similar a $A$, encontraríamos a los autovalores de $A$ en la diagonal de $Q^T A Q$  \n",
    "\n",
    "Con esto, ya tenemos una meta: tratar de encontrar esta $Q$ mágica que nos muestra los autovalores de $A$ de una manera tan linda, y para eso, utilizaremos el primer hecho fundamental. En la práctica **no** se conoce a  $Q$, pero se puede ir aproximando con la matirz $Q_m$ cuyas columnaas son los elementos de la baase buena $\\{q_1^m,...,q_n^m \\} $, que tiene la propiedad de que el espacio generado por sus primeras $k$ columnas es _casi_ $T_k$, para cualquier $k$. Se puede ver que mientras estos espacios se acercan, los elementos debajo de la diagonal superior de $Q_m^T A Q_m$ se van haciendo pequeños, con la misma velocidad que $A^m S_k$ se acerca a $T_k$, osea $(\\lambda_{k+1}/\\lambda_k)^m$ \n",
    "\n",
    "\n",
    "Con todo esto listo, podemos porfin comenzar a idear un algoritmo:\n",
    "1. Hacemos $S = \\langle e_1,...,e_n \\rangle = \\langle q_1^0,...,q_n^0 \\rangle$, que ya es una base ortogonal\n",
    "2. Calculamos $D_0= AS = A = [ Ae_1,...,Ae_n ]$. Sabemos que esta no es una base muy buena, por lo que calculamos más bien otra base $\\{q_1^1,...,q_n^1 \\}$ ortogonal. Esta corresponde a las columnas de la matriz $Q_1$ de la descomposición $A = Q_1 R_1$\n",
    "3. Miramos si las entradas bajo la diagonal de $Q_1^T A Q_1$ ya son suficientemente pequeñas como para considerarla triangular superior. Si eso sucede, retornamos las entradas de la diagonal de esa matriz. Si no, procedemos a\n",
    "4. Repetimos 2. y 3 : Hacemos $D_{i+1} = A D_i = [ A q_1^i,..., A q_n^i ]$, pero cambiamos esta base mala por la base $\\{q_1^{i+1},...,q_n^{i+1} \\}$, obtenida como los vectores columnas de $Q_{i+q}$ de la factorización $D_{i+1} = Q_{i+1} R_{i+1} $, y si $Q_{i+1} ^T A Q_{i+1}$ es suficientemente triangular superior, devolvemos los elementos de la diagonal\n",
    "\n",
    "Es valioso comparar esto con el planteamiento clásico del algoritmo QR. \n",
    "\n",
    "Al principio, hagamos $A_0 = A$, y $A_1 = Q_1^T A Q_1$. Pero habiamos hecho $A = Q_1 R_1 \\iff Q_1^T A = R_1$, por lo que realmente $A_1 = R_1 Q_1$. Después, para $D_2 = A Q_1$, hacemos factorización QR y encontramos que $ A Q_1 = Q_2 R_2 \\iff Q_2^T A = R_2 Q_1^T$. Hacemos $A_2 = Q_2^T A Q_2$, y $A_1 = Q_2'R_2' \\iff Q_2' ^T A_1 = R_2 '$. Como $D_1 = A Q_1 = Q_1 A_1 = Q_1 Q_2' R_2'$, por unicidad de la descomposición QR (estamos suponiendo que A no es singular), tenemos que $R_2' = R_2$ y que $Q_2 = Q_1 Q_2'$. Por tanto, $A_2 = Q_2^T A Q_2 = Q_2'^T Q_1 ^T  Q_1 Q_2' = Q_2'^T A_1 Q_2' = R_2' Q_2'$, y obtenemos la fórmula estándar de la iteración QR: $A_0 = A, A_k = Q_k' R_k', A_{k+1} = R_k' Q_k'$, equivalente a todo el trabjo que hemos hecho, lo que indica por que estas $A_i$ se acercan a una mtriz triaangular superior.\n",
    "\n",
    "Bien, que limitantes hemos impuesto sobre $A$ y $S$ para llegar a todo esto? Primero, como queremos que $S_k \\cap U_k = \\langle 0 \\rangle 0 \\quad \\forall k$, esto equivale a pedir que $\\langle e_1,...,e_k\\rangle \\cap \\langle v_{k+1},...,v_{n}\\rangle = \\langle 0 \\rangle \\quad \\forall k$, y por rgumentos de dimensionalidad de los $U_k$ y $S_k$, esto se cumple para prácticamente cualquier matriz A, son muy pocos los casos patológicos, asi que podemos tomar por sentada esta condición. Le pedimos a A que fuera simple, pero se puede mostrar que esta condición no es del todo necesaria, aunque se debe reemplazar a la $C$ en $(\\bigstar)$ por un polinomio $C(m)$, y si hay valores propios con módulo repetido, tambien se puede mostrar que converge, pero en ambos de estos casos, la convergencia es muy lenta, por lo que realmente no vale la pena utilizarlo en esta clase de matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272533e5-0477-4bb3-ab6a-5d496bc99dfb",
   "metadata": {},
   "source": [
    "Ahora, en el mundo numérico tenemos un problema: el método de ortogonalización de Gram-Schmidt clásico es bastante inestable numéricamente. En cada paso se deben calcular muchos productos y por lo menos una división, lo que tiende a amplificar los errores producidos por trabajar con aritmética de precisión finita a lo largo del proceso. Proponemos más bien una implementación de Gram-Schmidt modificada para hacer la descomposición QR. Comenzamos con una base $\\{a_1,...,a_n\\}$, e inicializamos otra $\\{v_1,...,v_n\\}$. Después, normalizamos $v_1$ para obtener el primer vector ortonormal $q_1 = v_1 / || v_1||$, y después, restamos de todos los $v_i$ para $i>1$ las proyecciones $\\mathcal{P}_{q_1}(v_i) = \\langle q_1,x_i \\rangle q_1$. Normalizamos $v_2$ para obtener $q_2 = v_2 /||v_2||$, y restamos de $v_i$ para $i>2$,  $\\mathcal{P}_{q_2}(v_i) = \\langle q_2,v_i \\rangle q_2$. Seguimos así hasta normalizar todos los $v_i$. Ya que tenemos los $q_i$, estos corresponden a las columnas de $Q$.\n",
    "Para ilustrar, si hacemos el proceso en una base $\\{ a_1,a_2,a_3 \\}$, obtenemos lo siguiente:\n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "v_1 = a_1\\\\\n",
    "v_2 = a_2 \\\\\n",
    "v_3 = a_3\\\\\n",
    "\\end{cases}& \\mapsto \\begin{cases}\n",
    "q_1 = v_1/||v_1||\\\\\n",
    "v_2 = a_2 - \\langle q_1,a_2 \\rangle q_1\\\\\n",
    "v_3 = a_3 - \\langle q_1,a_3 \\rangle q_1\n",
    "\\end{cases}\\\\ & \\mapsto \\begin{cases}\n",
    "q_1 = v_1/||v_1||\\\\\n",
    "q_2 = v_2/||v_2||\\\\\n",
    "v_3 = a_3 - \\langle q_1,a_3 \\rangle q_1 - \\langle q_2, (a_3 - \\langle q_1,a_3 \\rangle q_1) \\rangle q_2 \n",
    "\\end{cases}\\iff \\begin{cases}\n",
    "q_1 = v_1/||v_1||\\\\\n",
    "q_2 = v_2/||v_2||\\\\\n",
    "v_3 = a_3 - \\langle q_1,a_3 \\rangle q_1 - \\langle q_2,a_3\\rangle q_2 - \\langle q_2 ,\\langle q_1,a_3 \\rangle q_1\\rangle q_2\n",
    "\\end{cases}\\iff \\begin{cases}\n",
    "q_1 = v_1/||v_1||\\\\\n",
    "q_2 = v_2/||v_2||\\\\\n",
    "v_3 = a_3 - \\langle q_1,a_3 \\rangle q_1 - \\langle q_2,a_3\\rangle q_2\n",
    "\\end{cases}\\\\ &\\mapsto \\begin{cases}q_1 = v_1/||v_1||\\\\\n",
    "q_2 = v_2/||v_2||\\\\\n",
    "q_3 = v_3/||v_3|| \\end{cases}\n",
    "\\end{align*}\n",
    "Podemos ver que es equivalente al proceso estándar de Gram-Schmidt, pero por que es más estable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b1c00e57-6bd6-4bdc-9dc5-d6039920faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0]\n",
      " [0.0 0.6 -0.8]\n",
      " [0.0 0.8 0.6]]\n",
      "[[1.0 2.0 1.0]\n",
      " [0.0 5.0 2.2]\n",
      " [0.0 0.0 0.4]]\n"
     ]
    }
   ],
   "source": [
    "def QR_factor(A): #Gram-Schmidt modificado\n",
    "    \"\"\"\n",
    "    Entrada: A matriz de numpy invertible n x n\n",
    "    Proceso de ortogonalización de Gram-Smith modificado en los vectores columna de A\n",
    "    Salida: Q y R con Q ortogonal y R triangular superior con diagonal positiva tales que A = Q R\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    v = np.zeros((n,n)) \n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "   \n",
    "    for i in range(n):\n",
    "        v[:,i] = A[:,i] #Inicializamos A\n",
    "    #print(v)\n",
    "    for i in range(n):\n",
    "        R[i,i] = np.linalg.norm( v[:,i]) #Como esto es el coeficiente de q_i en la i-ésima fila, es la i,i-ésima componente de R\n",
    "        Q[:,i] = (1/R[i,i]) * v[:,i] #Normalizamos v_i\n",
    "        for k in range(i+1,n):\n",
    "            R[i,k] = Q[:,i] @ v[:,k] #Esto será el coeficiente de q_i en la k-ésima fila, por tanto es la i,k-ésima componente de R\n",
    "            v[:,k] = v[:,k] - R[i,k] * Q[:,i] #Actualizamos v_k restándole la proyección <q_i,v_k>q_i\n",
    "    return Q,R\n",
    "\n",
    "#Toy example:\n",
    "a_1 = [1,0,0]\n",
    "a_2 = [2,3,4]\n",
    "a_3 = [1,1,2]\n",
    "A=np.array([a_1,a_2,a_3]).transpose()\n",
    "with np.printoptions(precision=6, suppress=True, formatter={'float': '{:0.1f}'.format}, linewidth=100):\n",
    "    print(QR_factor(A)[0])\n",
    "    print(QR_factor(A)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2a7d96f-f6d2-4801-a4ad-8edf34bd9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottom_half(A):\n",
    "    \"\"\"\n",
    "    Almacena las componentes debajo de la diagonal superior de A\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    L=[]\n",
    "    for i in range(n):\n",
    "        T = list(A[i,:])\n",
    "        for k in range(i):   \n",
    "            L.append(T[k])\n",
    "    return L\n",
    "\n",
    "def QR_algor(A,max_iteraciones=10000,tol=10e-8):\n",
    "    \"\"\"\n",
    "    Entrada: A matriz de numpy que debería ser simple e invertible, con autovalores distintos en módulo\n",
    "             max_iteraciones: tope de recursion. si se alcanza se retorna error\n",
    "             tol: tolerancia, que tan cerca de ser triangular superior es la matriz A^(n) en el paso n\n",
    "    Salida: B matriz (casi) triangular superior similar a A, con sus autovalores ordenados por su módulo en la diagonal, lista con los autovalores, iteraciones necesarias\n",
    "    \"\"\"\n",
    "    iteracion = 0\n",
    "    n=A.shape[0]\n",
    "    while iteracion < max_iteraciones:\n",
    "        iteracion += 1\n",
    "        Q,R = QR_factor(A) #Calculamos la ddescomposición A_k = Q'_k R'_k\n",
    "        A = R @ Q  #Y hacemos A_(k+1) = R'_k Q'_k. Sabemos que sto es equivalente a Q_k^T A Q_k obtenidas por iteración simultánea, y converge a una matriz triangular superior\n",
    "        L = [abs(l) for l in bottom_half(A)] #Para chequear si estamos suficientemente cerca, tomamos los valores absolutos de la mitad triangular inferior de A...\n",
    "        if max(L) < tol: #Y aceptamos si todos son menores que la tolerancia\n",
    "            Autovalores = [A[i][i] for i in range(n)] #Almacenamos los autovalores aqui\n",
    "            return A,Autovalores,iteracion\n",
    "    print(A)\n",
    "    print(Q)\n",
    "    print(Q.transpose() @ Q)\n",
    "    return \"Iteraciones máximas alcanzadas\" #Error si no converge suficientemente rápido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44489b-50e7-493a-9778-6aa4a42c7441",
   "metadata": {},
   "source": [
    "Bien, solo nos falta describir como se comportan los errores y la convergencia. Los errores se producen únicamente por artmética de precisión finita, en especial en la descomposición QR de cada $A_i$. Existen métodos alternativos más estables par encontrarla, como utilizando reflexiones de Householder o rotaciones de Givens, pero es valioso apreciar la relación entre esta matriz ortogonal $Q$ y el proceso de ortogonalización de las columnas de $A_i$, además de ser bastante sencillo de implementar. \n",
    "\n",
    "Ahora, que tan rápido converge esto a una matriz triangular superior? Sabemos que por $(\\bigstar)$, esta debe estar acotada por $C \\cdot |\\lambda_{i+1} / \\lambda_i|^m $ para el $i \\in \\{ 1, 2,...,n-1\\}$que haga más cercana a 1 esta razón. Por tanto, si los autovalores de $A$ no están bien espaciados, la convergencia se puede volver demasiado lenta para ser útil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
